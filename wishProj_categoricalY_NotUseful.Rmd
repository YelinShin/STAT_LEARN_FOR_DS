---
title: "954:534 Wish Project"
author: "Kanya Kreprasertkul, Yelin Shin and Zhe Ren"
output:
  html_document:
    df_print: paged
---

```{r, message = FALSE}
options(warn = -1)
library(dplyr)
library(tidyr)
#library(tidyverse)
library(GGally)
library(plotly)
library(cowplot)
library(ggcorrplot)
library(stringr)
```

### Data pre-processing
```{r, results='hide', echo=TRUE}
wish <- read.csv('summer-products-with-rating-and-performance_2020-08.csv')

#dropping unnecessary columns 
drops <- c('title', 'tags', 'crawl_month', 'theme', 'product_id', 'product_picture', 'product_url', 'merchant_id', 'merchant_profile_picture', 'merchant_info_subtitle', 'merchant_name', 'merchant_title', 'urgency_text', 'title_orig', 'shipping_option_name', 'currency_buyer')
wish <- wish[, !(names(wish) %in% drops)]

#convert NA to 0
wish$has_urgency_banner <- as.integer(wish$has_urgency_banner)
wish$has_urgency_banner[which(is.na(wish$has_urgency_banner))] <- 0
wish$rating_five_count[which(is.na(wish$rating_five_count))] <- 0
wish$rating_four_count[which(is.na(wish$rating_four_count))] <- 0
wish$rating_three_count[which(is.na(wish$rating_three_count))] <- 0
wish$rating_two_count[which(is.na(wish$rating_two_count))] <- 0
wish$rating_one_count[which(is.na(wish$rating_one_count))] <- 0
wish$rating[which(wish$rating_count == 0)] <- 0

# cleaning size and color option
wish <- wish %>%
  mutate(product_variation_size_id = tolower(product_variation_size_id)) %>%
  mutate(product_variation_size_id = gsub(pattern = '.', replacement = '',
                                          x = product_variation_size_id, fixed = TRUE)) %>%
  mutate(product_variation_size_id = gsub(pattern = '(size-*)|(size)', replacement = '',
                                          x = product_variation_size_id)) %>%
  mutate(product_variation_size_id = gsub(pattern = '.+[-]', replacement = '',
                                          x = product_variation_size_id)) %>%
  mutate(product_variation_size_id = ifelse(grepl(pattern = 'xl',product_variation_size_id),
                                            'xl', product_variation_size_id)) %>%
  mutate(product_variation_size_id = ifelse(grepl(pattern = 'xs', product_variation_size_id),
                                            'xs', product_variation_size_id)) %>%
  mutate(product_variation_size_id = str_replace(product_variation_size_id, ' ', '')) %>%
  mutate(product_variation_size_id = ifelse(product_variation_size_id %in% c('s', 'xs', 'm', 'l', 'xl'),product_variation_size_id, 'One-sized'))
wish <- wish %>% 
    mutate(product_color = tolower(product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'red|burgundy|claret|wine|jasper', product_color),
                                  'red', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'blue|navy', product_color),
                                  'blue', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'white', product_color),
                                  'white', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'green|army', product_color),
                                  'green', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'black', product_color),
                                  'black', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'yellow|leopard|gold', product_color),
                                  'yellow', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'pink|rose', product_color),
                                  'pink', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'grey|gray|silver', product_color),
                                  'gray', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'purple|violet', product_color),
                                  'purple', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'orange|apricot', product_color),
                                  'orange', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'beige|nude|ivory|coffee|brown|khaki|camel',
                                        product_color), 'khaki', product_color)) %>%
    mutate(product_color = ifelse(grepl(pattern = 'floral|multicolor|camouflage|rainbow|star',
                                        product_color), 'multicolor', product_color))

#name blank category 
wish['product_color'][wish['product_color'] == ''] <- 'Not defined'
wish['origin_country'][wish['origin_country'] == ''] <- 'Not defined'

#shipping_is_express has too many zero, so we decided to exclude this column
wish <- select(wish, -c(shipping_is_express))

#Only 7 colors have more than 100 records so We decided to keep only 8 factors of color, i.e. black, white, blue, red, green, yellow, pink and others.
color_list <- c('black', 'white', 'blue', 'red', 'green', 'yellow', 'pink')
wish$product_color[!(wish$product_color %in% color_list)] <- 'others'

wish %>% 
  group_by(product_color) %>%
  summarise(no_rows = length(product_color)) %>%
  arrange(desc(no_rows)) %>%
  filter(no_rows > 100)

#We decided to change origin to CN and others.
wish$origin_country <- as.character(wish$origin_country)
wish$origin_country[which(wish$origin_country != 'CN')] <- 'others'
wish$origin_country[is.na(wish$origin_country)] <- 'others'

wish %>% 
  group_by(origin_country) %>%
  summarise(no_rows = length(origin_country)) %>%
  arrange(desc(no_rows)) 

#convert column name to short version
origin_colname <- colnames(wish)
colnames(wish) <- c('price', 'retail', 'sold_ct', 'ad_boost', 'rate', 'rate_ct', 'rate5', 'rate4', 'rate3', 'rate2', 'rate1', 'badge_ct', 'bg_local', 'bg_quality', 'bg_fastship', 'color', 'size', 'inventory', 'ship_price', 'able_country', 'total_invent', 'has_bg_urgency', 'origin', 'seller_rate_ct', 'seller_rate', 'has_seller_propic')

```

```{r}
library(corrplot)
# finding correlation between numeric columns and charges

numeric.column <- sapply(wish, is.numeric)
corr <- cor(wish[, numeric.column]) #, use = 'pairwise.complete.obs'
corrplot(corr, method = 'color')

```

```{r}
#convert the y (sold_ct) to categorical. Also since it is unbalanced we group some category together.
table(wish['sold_ct']) # very unbalaned
wish_cate <- wish
wish_cate$sold_ct_cate <- wish_cate$sold_ct
wish_cate$sold_ct_cate[which(wish_cate$sold_ct <= 50)] <- 'below 50'
wish_cate$sold_ct_cate[which(wish_cate$sold_ct >= 20000)] <- 'above 20K'
wish_cate <- select(wish_cate, -sold_ct)
wish_cate$sold_ct_cate <- as.factor(wish_cate$sold_ct_cate)
wish_cate$color <- as.factor(wish_cate$color)
wish_cate$size <- as.factor(wish_cate$size)
wish_cate$origin <- as.factor(wish_cate$origin)
table(wish_cate$sold_ct_cate) # much better

x1 <- factor(wish_cate$sold_ct_cate, levels = c("below 50", "100", "1000", "5000", "10000", "above 20K"))
tb <- table(x1)
barplot(tb, names.arg = row.names(tb), cex.names = 0.8, main = "sold_ct as categorical", las = 2)

wish_cate$rate5_pct <- wish_cate$rate5/wish_cate$rate_ct
wish_cate$rate4_pct <- wish_cate$rate4/wish_cate$rate_ct
wish_cate$rate3_pct <- wish_cate$rate3/wish_cate$rate_ct
wish_cate$rate2_pct <- wish_cate$rate2/wish_cate$rate_ct
wish_cate$rate1_pct <- wish_cate$rate1/wish_cate$rate_ct

drops <- c('rate_ct', 'rate5', 'rate4', 'rate3', 'rate2', 'rate1')
wish_cate <- wish_cate[, !(names(wish_cate) %in% drops)]
# str(wish_simplify)

str(wish_cate)

```


#### 80:20 split for train and test set
```{r}
set.seed(123)
train_rows <- sample(1:nrow(wish), 0.8 * nrow(wish))
wish.train <- wish_cate[train_rows, ] # wish training set
wish.test <- wish_cate[-train_rows, ]
```

### Tree Models

```{r}
library(tree)

set.seed(123)

tree.wish <- tree(sold_ct_cate ~ ., data = wish.train) 
summary(tree.wish)
tree.pred <- predict(tree.wish, wish.test, type = "class")

table(tree.pred, wish.test$sold_ct_cate)

print("Misclassification error rate on test set: ")

1 - ((table(tree.pred, wish.test$sold_ct_cate)[1] + table(tree.pred, wish.test$sold_ct_cate)[8] + table(tree.pred, wish.test$sold_ct_cate)[15] + table(tree.pred, wish.test$sold_ct_cate)[22] + table(tree.pred, wish.test$sold_ct_cate)[29] + table(tree.pred, wish.test$sold_ct_cate)[36]) / nrow(wish.test))
```

### Bagging

```{r}
library(randomForest)
set.seed(123)

bag.wish <- randomForest(sold_ct_cate ~ ., data = wish.train, mtry = 25, importance = TRUE, na.action=na.exclude)
bag.wish

bag.pred <- predict(bag.wish, wish.test)

table(bag.pred, wish.test$sold_ct_cate)

print("Misclassification error rate on test set: ")

1 - ((table(bag.pred, wish.test$sold_ct_cate)[1] + table(bag.pred, wish.test$sold_ct_cate)[8] + table(bag.pred, wish.test$sold_ct_cate)[15] + table(bag.pred, wish.test$sold_ct_cate)[22] + table(bag.pred, wish.test$sold_ct_cate)[29] + table(bag.pred, wish.test$sold_ct_cate)[36]) / nrow(wish.test))
```

### Random forest

```{r}
set.seed(123)

rf.wish <- randomForest(sold_ct_cate ~ ., data = wish.train, mtry = 25 / 3, importance = TRUE, na.action=na.exclude)
rf.wish

rf.pred <- predict(rf.wish, wish.test)

table(rf.pred, wish.test$sold_ct_cate)

print("Misclassification error rate on test set: ")

1 - ((table(rf.pred, wish.test$sold_ct_cate)[1] + table(rf.pred, wish.test$sold_ct_cate)[8] + table(rf.pred, wish.test$sold_ct_cate)[15] + table(rf.pred, wish.test$sold_ct_cate)[22] + table(rf.pred, wish.test$sold_ct_cate)[29] + table(rf.pred, wish.test$sold_ct_cate)[36]) / nrow(wish.test))
```

### GBM

```{r}
set.seed(123)

library(h2o)
h2o.init()

wish.train.h2o <- as.h2o(wish.train)
wish.test.h2o <- as.h2o(wish.test)
predictors <- c(colnames(wish.train)[1:length(wish.train) - 1])
response <- "sold_ct_cate"

# Build and train the model:
gbm.wish <- h2o.gbm(x = predictors,
                    y = response,
                    nfolds = 5,
                    distribution = "multinomial",
                    keep_cross_validation_predictions = TRUE,
                    training_frame = wish.train.h2o)

h2o.confusionMatrix(gbm.wish)
print("Misclassification error rate on training set: ")
h2o.confusionMatrix(gbm.wish)['Totals','Error']

h2o.confusionMatrix(gbm.wish, wish.test.h2o)
print("Misclassification error rate on test set: ")
h2o.confusionMatrix(gbm.wish, wish.test.h2o)['Totals','Error']
```

### Multinomial regression

```{r}
set.seed(123)
library(nnet)
multinomial.mod <- multinom(sold_ct_cate ~ ., data = wish.train) 
summary(multinomial.mod)
multinomial.pred_train <- predict(multinomial.mod, wish.train) 
multinomial.pred_test <- predict(multinomial.mod, wish.test)
# training error
print("Misclassification rate on the training set:")
mean(as.character(multinomial.pred_train) != as.character(wish.train$sold_ct_cate))
# test error
print("Misclassification rate on the test set:")
mean(as.character(multinomial.pred_test) != as.character(wish.test$sold_ct_cate))
```


### SVM

```{r}
library(e1071)

set.seed(123)

tuned <- tune(svm, sold_ct_cate ~ ., data = wish.train, kernel = "linear", ranges = list(cost = append(seq(0.01, 10, by = 0.5), 10)))
summary(tuned) # cost = 3.51 is the best

lin.svm <- svm(sold_ct_cate ~ ., kernel = "linear", type = "C-class", data = wish.train, cost = 3.51)

train_pred <- predict(lin.svm, wish.train)
table <- table(wish.train$sold_ct_cate, train_pred)

print("training error with cost = 3.51: ")
(sum(table)-sum(diag(table))) / (sum(table))

test_pred <- predict(lin.svm, wish.test)
table <- table(wish.test$sold_ct_cate, test_pred)

print("test error with cost = 3.51: ")
(sum(table)-sum(diag(table))) / (sum(table))

# we cannot plot SVM classification plot since we have more than 2 columns
table(wish.test$sold_ct_cate, test_pred)
```

```{r}
set.seed(123)

tuned <- tune(svm, sold_ct_cate ~ ., data = wish.train, kernel = "radial", ranges = list(cost = append(seq(0.01, 10, by = 0.5), 10)))
summary(tuned) # cost = 10 is best

table(wish.test$sold_ct_cate, test_pred)

rad.svm <- svm(sold_ct_cate ~ ., kernel = "radial", data = wish.train, cost = 10)

train_pred <- predict(rad.svm, wish.train)
table <- table(wish.train$sold_ct_cate, train_pred)

print("radical svm - training error with cost = 10: ")
(sum(table)-sum(diag(table))) / (sum(table))

test_pred <- predict(rad.svm, wish.test)
table <- table(wish.test$sold_ct_cate, test_pred)

print("radical svm - test error with cost = 10: ")
(sum(table)-sum(diag(table))) / (sum(table))

table(wish.test$sold_ct_cate, test_pred)

```

The result shows that it result overfitting. (traing error is getting low, but test error is getting higher)

```{r}
tune.poly <- tune(svm, sold_ct_cate ~ ., data = wish.train, kernel = "poly", degree = 3, ranges = list(cost = append(seq(0.01, 10, by = 0.5), 10)))
summary(tuned) # cost = 10 is best

poly.svm <- svm(sold_ct_cate ~ ., kernel = "poly", data = wish.train, degree = 3, cost = 10)

train_pred <- predict(poly.svm, wish.train)
table <- table(wish.train$sold_ct_cate, train_pred)

print("poly svm - training error with cost = 10: ")
(sum(table)-sum(diag(table))) / (sum(table))

test_pred <- predict(poly.svm, wish.test)
table <- table(wish.test$sold_ct_cate, test_pred)

print("poly svm - test error with cost = 10: ")
(sum(table)-sum(diag(table))) / (sum(table))

table(wish.test$sold_ct_cate, test_pred)
```


### XGBoost
```{r, message = FALSE}
library(xgboost)
# Create numeric labels with one-hot encoding
train_labs <- as.numeric(wish.train$sold_ct_cate) - 1
val_labs <- as.numeric(wish.test$sold_ct_cate) - 1

new_train <- model.matrix(~ . + 0, data = subset(wish.train, select = -sold_ct_cate))
new_val <- model.matrix(~ . + 0, data = subset(wish.test, select = -sold_ct_cate))

# Prepare matrices
xgb_train <- xgb.DMatrix(data = new_train, label = train_labs)
xgb_val <- xgb.DMatrix(data = new_val, label = val_labs)

params <- list(booster = "gbtree", objective = "multi:softprob", num_class = 8, eval_metric = "mlogloss")

# Calculate # of folds for cross-validation
xgbcv <- xgb.cv(params = params, data = xgb_train, nrounds = 100, nfold = 5, showsd = TRUE, stratified = TRUE, print_every_n = 10, early_stop_round = 20, maximize = FALSE, prediction = TRUE)

```

```{r}
# Function to compute classification error
classification_error <- function(conf_mat) {
  conf_mat = as.matrix(conf_mat)
  
  error = 1 - sum(diag(conf_mat)) / sum(conf_mat)
  
  return (error)
}

# Mutate xgb output to deliver hard predictions
xgb_train_preds <- data.frame(xgbcv$pred) %>% mutate(max = max.col(., ties.method = "last"), label = train_labs + 1)

# Examine output
head(xgb_train_preds)

xgb_conf_mat <- table(true = train_labs + 1, pred = xgb_train_preds$max)

# Error 
cat("XGB Training Classification Error Rate:", classification_error(xgb_conf_mat), "\n")

# predicting / testing on test dataset
xgb_model <- xgb.train(params = params, data = xgb_train, nrounds = 100)

# Predict for validation set
xgb_val_preds <- predict(xgb_model, newdata = xgb_val)

xgb_val_out <- matrix(xgb_val_preds, nrow = 8, ncol = length(xgb_val_preds) / 8) %>% 
               t() %>%
               data.frame() %>%
               mutate(max = max.col(., ties.method = "last"), label = val_labs + 1) 

# Confustion Matrix
xgb_val_conf <- table(true = val_labs + 1, pred = xgb_val_out$max)

cat("XGB Validation Classification Error Rate:", classification_error(xgb_val_conf), "\n")

```


## SMOTE 
```{r}



```


## Random Forest with class weight 
```{r}
# adapt on random forest 
# https://www.rdocumentation.org/packages/iRF/versions/2.0.0/topics/randomForest 
set.seed(123)

bag.wish_over <- randomForest(sold_ct_cate ~ ., data = wish.train, mtry = 25, importance = TRUE, classwt = c(0.2,0.2,0.2,0.2,0.2,0.2))
bag.wish_over

bag.pred_over <- predict(bag.wish_over, wish.test)

table(bag.pred_over, wish.test$sold_ct_cate)

print("Misclassification error rate on test set: ")

1 - ((table(bag.pred_over, wish.test$sold_ct_cate)[1] + table(bag.pred_over, wish.test$sold_ct_cate)[8] + table(bag.pred_over, wish.test$sold_ct_cate)[15] + table(bag.pred_over, wish.test$sold_ct_cate)[22] + table(bag.pred_over, wish.test$sold_ct_cate)[29] + table(bag.pred_over, wish.test$sold_ct_cate)[36]) / nrow(wish.test))
```



```{r}
library(rfUtilities)
# https://www.rdocumentation.org/packages/rfUtilities/versions/2.1-5/topics/rf.crossValidation 
rf.cv <- rf.crossValidation(bag.wish_over,  wish.train, p=0.10, n=99, ntree=501) 

 # Plot cross validation versus model producers accuracy
par(mfrow=c(1,2)) 
 plot(rf.cv, type = "cv", main = "CV producers accuracy")
 plot(rf.cv, type = "model", main = "Model producers accuracy")

 # Plot cross validation versus model oob
par(mfrow=c(1,2)) 
 plot(rf.cv, type = "cv", stat = "oob", main = "CV oob error")
 plot(rf.cv, type = "model", stat = "oob", main = "Model oob error")

```



## cross validation
```{r}
# https://www.geeksforgeeks.org/cross-validation-in-r-programming/ 
library(caret) 
set.seed(125)  
  

# value of K equal to 10 
train_control <- trainControl(method = "cv", 
                              number = 10) 
  
bag.wish_over <- train(sold_ct_cate ~., data = wish.train,  
               method = "rf", 
               trControl = train_control) 
  
# printing model performance metrics 
# along with other details 
print(bag.wish_over)

```


## re-categorize the target variable into 3 groups: low sells, good sells, great sells

```{r}
#convert the y (sold_ct) to categorical. Also since it is unbalanced we group some category together.



wish_simplify <- wish
wish_simplify$sold_ct_3 <- wish_simplify$sold_ct
wish_simplify$sold_ct_3[which(wish_simplify$sold_ct <= 100)] <- 'below 100'
wish_simplify$sold_ct_3[which(wish_simplify$sold_ct >= 10000)] <- 'above 10K'
wish_simplify$sold_ct_3[which(wish_simplify$sold_ct > 100 & wish_simplify$sold_ct < 10000)] <- 'between 100 and 10k'
wish_simplify <- select(wish_simplify, -sold_ct)
wish_simplify$sold_ct_3 <- as.factor(wish_simplify$sold_ct_3)
wish_simplify$color <- as.factor(wish_simplify$color)
wish_simplify$size <- as.factor(wish_simplify$size)
wish_simplify$origin <- as.factor(wish_simplify$origin)
table(wish_simplify$sold_ct_3) # much better

```


```{r}

x1 <- factor(wish_simplify$sold_ct_3, levels = c("below 100", "between 100 and 10k","above 10K"))
tb <- table(x1)
barplot(tb, names.arg = row.names(tb), cex.names = 0.8, main = "sold_ct as categorical", las = 2)
# now the dataset is more balanced
```


```{r}
wish_simplify$rate5_pct <- wish_simplify$rate5/wish_simplify$rate_ct
wish_simplify$rate4_pct <- wish_simplify$rate4/wish_simplify$rate_ct
wish_simplify$rate3_pct <- wish_simplify$rate3/wish_simplify$rate_ct
wish_simplify$rate2_pct <- wish_simplify$rate2/wish_simplify$rate_ct
wish_simplify$rate1_pct <- wish_simplify$rate1/wish_simplify$rate_ct

drops <- c('rate_ct', 'rate5', 'rate4', 'rate3', 'rate2', 'rate1')
wish_simplify <- wish_simplify[, !(names(wish_simplify) %in% drops)]
str(wish_simplify)
```




# 80:20 split
```{r}
set.seed(123)
train_rows <- sample(1:nrow(wish), 0.8 * nrow(wish))
wish.train <- wish_simplify[train_rows, ] # wish training set
wish.test <- wish_simplify[-train_rows, ]

```


### cross validation for different models 
```{r}



```


```{r}
# A naive model 
rpart.model <- rpart(sold_ct_3~., data=wish.train, method="class")
print(rpart.model)

rcart.prediction <- predict(rpart.model, newdata=wish.test, type="class")
confusion.matrix <- table(wish.test$sold_ct_3, rcart.prediction)
print(confusion.matrix)

accuracy.percent <- 100*sum(diag(confusion.matrix))/sum(confusion.matrix)
print(paste("accuracy:",accuracy.percent,"%"))
```


# another kind of decision tree
```{r}
require(tree)
tree.wish = tree(sold_ct_3~., data=wish.train)
summary(tree.wish)
plot(tree.wish)
text(tree.wish, pretty = 0)
```



#with train and test dataset 
```{r}
tree.pred = predict(tree.wish, wish.test, type="class")
with(wish.test,table(wish.test$sold_ct_3, tree.pred))

confusion.matrix <- table(wish.test$sold_ct_3, tree.pred)
print(confusion.matrix)

accuracy.percent <- 100*sum(diag(confusion.matrix))/sum(confusion.matrix)
print(paste("accuracy:",accuracy.percent,"%"))

```


#cv 
```{r}
cv.wish.tree = cv.tree(tree.wish, FUN = prune.misclass)
cv.wish.tree
plot(cv.wish.tree)
# from the plot we can see the misclassification rate doesn't go down after n=3

```


```{r}

prune.tree = prune.misclass(tree.wish, best = 3)
plot(prune.tree)
text(prune.tree, pretty=0)

```



```{r}

tree.pred = predict(prune.tree, wish.test, type="class")

confusion.matrix <- table(wish.test$sold_ct_3, tree.pred)
print(confusion.matrix)

accuracy.percent <- 100*sum(diag(confusion.matrix))/sum(confusion.matrix)
print(paste("accuracy:",accuracy.percent,"%"))
# the result is satisfying- pruning doesn't hurt the prediction 
```


## Random Forest
```{r}
# wish.train$sold_ct_3
# wish.train$sold_ct_3[which(is.na(wish.train$sold_ct_3))]
```

```{r}
# rf.wish = randomForest(sold_ct_3~., data = wish.train)
# https://stackoverflow.com/questions/38250440/error-in-na-fail-default-missing-values-in-object-but-no-missing-values
# https://www.kaggle.com/nsecord/gbm-parameter-estimation
rf.wish <- randomForest(sold_ct_3 ~ ., data = wish.train, mtry = 25, importance = TRUE,na.action=na.roughfix)
rf.wish
```



```{r}

# https://www.geeksforgeeks.org/cross-validation-in-r-programming/ 
library(caret) 
set.seed(125)  
  

# value of K equal to 10 
train_control <- trainControl(method = "cv", 
                              number = 10) 
  
rf.wish.cv <- train(sold_ct_3 ~., data = wish.train,  
               method = "rf", ,na.action=na.roughfix,
               trControl = train_control) 
  
# printing model performance metrics 
# along with other details 
print(rf.wish.cv)

```

over sampling 
```{r}
set.seed(123)

rf.wish.over <- randomForest(sold_ct_3 ~ ., data = wish.train, mtry = 24, importance = TRUE, classwt = c(0.33,0.33,0.33), na.action=na.roughfix)
rf.wish.over

rf.pred.over <- predict(rf.wish.over, wish.test)

# table(rf.pred.over, wish.test$sold_ct_3)

confusion.matrix <- table(wish.test$sold_ct_3, rf.pred.over)
print(confusion.matrix)

accuracy.percent <- 100*sum(diag(confusion.matrix))/sum(confusion.matrix)
print(paste("accuracy:",accuracy.percent,"%"))

```


To tune the model, we try different `mtry`. now mtry = 24 <br>
it reaches the highest accuracy when mtry = 3 or 6, within the smallest mtry range, 
```{r}

accuracy = double(24)
for(mtry in 1:24){
  fit = randomForest(sold_ct_3~., data = wish.train, mtry=mtry,na.action=na.roughfix)
  pred = predict(fit, wish.test)
  confusion.matrix <- table(wish.test$sold_ct_3, pred)
  accuracy[mtry] = 100*sum(diag(confusion.matrix))/sum(confusion.matrix)
}

accuracy
matplot(1:mtry, accuracy, pch = 23, col= "blue", type = "b", ylab="Prediction Accuracy")
legend("topright", legend =  "Test", pch = 23, col = "blue")

# it reaches the highest accuracy when mtry = 4 or 5, within the smallest mtry range 

```

then we go with mtry = 5<br>
when oversampled, the test accuracy is 77.05%. 
```{r}
set.seed(123)

rf.wish.over <- randomForest(sold_ct_3 ~ ., data = wish.train, mtry = 5, importance = TRUE, classwt = c(0.33,0.33,0.33), na.action=na.roughfix)
rf.wish.over

rf.pred.over <- predict(rf.wish.over, wish.test)

# table(rf.pred.over, wish.test$sold_ct_3)

confusion.matrix <- table(wish.test$sold_ct_3, rf.pred.over)
print(confusion.matrix)

accuracy.percent <- 100*sum(diag(confusion.matrix))/sum(confusion.matrix)
print(paste("accuracy:",accuracy.percent,"%"))

```


```{r}
set.seed(123)

rf.wish.over <- randomForest(sold_ct_3 ~ ., data = wish.train, mtry = 5, importance = TRUE, classwt = c(0.1,0.9,0.9), na.action=na.roughfix)
rf.wish.over

rf.pred.over <- predict(rf.wish.over, wish.test)

# table(rf.pred.over, wish.test$sold_ct_3)

confusion.matrix <- table(wish.test$sold_ct_3, rf.pred.over)
print(confusion.matrix)
print(confusion.matrix[1,1])

accuracy.percent <- 100*sum(diag(confusion.matrix))/sum(confusion.matrix)
above10k.precent <- 100*confusion.matrix[1,1]/sum(confusion.matrix[1,])
print(paste("Test accuracy:",accuracy.percent,"%"))
print(paste("Above 10k accuracy:",above10k.precent,"%"))

```

When not oversampled, the test accuracy is 78.36%, even slightly higher than the oversampled data.  
```{r}
set.seed(123)

rf.wish.over <- randomForest(sold_ct_3 ~ ., data = wish.train, mtry = 5, importance = TRUE, na.action=na.roughfix)
rf.wish.over

rf.pred.over <- predict(rf.wish.over, wish.test)

# table(rf.pred.over, wish.test$sold_ct_3)

confusion.matrix <- table(wish.test$sold_ct_3, rf.pred.over)
print(confusion.matrix)

accuracy.percent <- 100*sum(diag(confusion.matrix))/sum(confusion.matrix)
above10k.precent <- 100*confusion.matrix[1,1]/sum(confusion.matrix[1,])
print(paste("Test accuracy:",accuracy.percent,"%"))
print(paste("Above 10k accuracy:",above10k.precent,"%"))
```

##Boosting 
```{r}

boost.wish = gbm(sold_ct_3~., data = wish.train, distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.wish)

```



```{r}
plot(boost.wish,i="rate_ct")
plot(boost.wish,i="rate4")


```

##Modeling building with H20
```{r}
library(h2o)
# train_soldct <- select(wish.train, -c(sold_ct_3))
# test_soldct <- select(wish.test, -c(sold_ct_3))
# train1 <- as.h2o(train_soldct)
# test1 <- as.h2o(test_soldct)
train1 <- as.h2o(wish.train)
test1 <- as.h2o(wish.test)
```


```{r}
automl1 <- h2o.automl(y='sold_ct_3',training_frame=train1,max_models=15)

a1 <- automl1@leaderboard
print(a1,n=nrow(a1))

```



```{r}
pred1 <- h2o.predict(automl1, test1)
mse_aml1 <- mean((test1$sold_ct_3 - pred1)^2) 
mse_aml1


```


# GLM - H2o 
don't want to dive deeper - the performance is disappointing. 
```{r}
# h2o.init(nthreads=-1, max_mem_size="2G")
# h2o.removeAll() ## clean slate - just in case the cluster was already running

# train1 <- as.h2o(wish.train)
# test1 <- as.h2o(wish.test)

wish.h2o <- as.h2o(wish_simplify)
data = h2o.splitFrame(wish.h2o,ratios=c(.79,.01),destination_frames = c("train","test","valid"))
names(data) 
```





```{r}
myX = colnames(select(wish.train, -c(sold_ct_3)))
m1 = h2o.glm(training_frame = wish.train_sid_98d3_9, validation_frame = test1, x = myX, y = 'sold_ct_3',family='multinomial',solver='L_BFGS')
h2o.confusionMatrix(m1, valid=TRUE)


```


```{r}
# write.csv(wish_simplify,"wish_simplify.csv", row.names = FALSE)
# path = system.file("/Users/RZhe/Documents/GitHub/STAT_LEARN_FOR_DS/wish_simplify.csv", package = "h2o")
# h2o_df = h2o.importFile(path)

# h2o.init(ip = "localhost", port = 54321, startH2O = TRUE)
# wish.path = system.file("wish_simplify.csv", package = "h2o")
wish.h2o = h2o.importFile("wish_simplify.csv")
```



```{r}
h2o.glm(y = "sold_ct_3", x = myX, training_frame = wish.h2o,
        family='multinomial',solver='L_BFGS', nfolds = 5)

# family='multinomial',solver='L_BFGS', lambda = 0
```



```{r}
h2o.glm(y = "sold_ct_3", x = myX, training_frame = wish.h2o,
        family='multinomial',solver='L_BFGS', nfolds = 5, lambda = 0)
# https://www.rdocumentation.org/packages/h2o/versions/3.32.0.1/topics/h2o.glm

```
I don't think we should use GLM given the high error rate. 

## SVM 
```{r}

set.seed(123)

tuned <- tune(svm, sold_ct_3 ~ ., data = wish.train, kernel = "linear", ranges = list(cost = append(seq(0.01, 10, by = 0.5), 10)))
summary(tuned)

```



```{r}
# https://www.rdocumentation.org/packages/e1071/versions/1.7-4/topics/svm
lin.svm <- svm(sold_ct_3 ~ ., kernel = "linear", type = "C-class", data = wish.train, cost = 2.01)

train_pred <- predict(lin.svm, wish.train)
table <- table(wish.train$sold_ct_3, train_pred)

print("training error with cost = 2.01: ")
(sum(table)-sum(diag(table))) / (sum(table))

test_pred <- predict(lin.svm, wish.test)
# table <- table(wish.test$sold_ct_3, test_pred)

print("test error with cost = 2.01: ")
(sum(table)-sum(diag(table))) / (sum(table))

# table(wish.test$sold_ct_3, test_pred)

```



##XGBoost 

```{r}
train_labs <- as.numeric(wish.train$sold_ct_3) -1
val_labs <- as.numeric(wish.test$sold_ct_3) -1

new_train <- model.matrix(~ . + 0, data= subset(wish.train, select = -sold_ct_3))
new_val <- model.matrix(~ . + 0, data = subset(wish.test, select = -sold_ct_3))

# nrow(wish.train)
nrow(new_train)
# nrow(select(wish.train, -c(sold_ct_3)))
# length(as.numeric(wish.train$sold_ct_3))
# nrow(as.numeric(wish.train$sold_ct_3))
length(train_labs)

# Prepare matrices
xgb_train <- xgb.DMatrix(data = new_train, label = train_labs)
xgb_val <- xgb.DMatrix(data = new_val, label = val_labs)

x_prep = as.matrix(subset(wish.train, select = -sold_ct_3))
xgb_train <- xgb.DMatrix(data = x_prep, label = wish.train$sold_ct_3)


params <- list(booster = "gbtree", objective = "multi:softprob", num_class = 8, eval_metric = "mlogloss")

# Calculate # of folds for cross-validation
xgbcv <- xgb.cv(params = params, data = xgb_train, nrounds = 100, nfold = 5, showsd = TRUE, stratified = TRUE, print_every_n = 10, early_stop_round = 20, maximize = FALSE, prediction = TRUE)

# Function to compute classification error
classification_error <- function(conf_mat) {
  conf_mat = as.matrix(conf_mat)
  
  error = 1 - sum(diag(conf_mat)) / sum(conf_mat)
  
  return (error)
}

# Mutate xgb output to deliver hard predictions
xgb_train_preds <- data.frame(xgbcv$pred) %>% mutate(max = max.col(., ties.method = "last"), label = train_labs + 1)

# Examine output
head(xgb_train_preds)


xgb_conf_mat <- table(true = train_labs + 1, pred = xgb_train_preds$max)

# Error 
cat("XGB Training Classification Error Rate:", classification_error(xgb_conf_mat), "\n")

```


```{r}

# predicting / testing on test dataset
xgb_model <- xgb.train(params = params, data = xgb_train, nrounds = 100)

# Predict for validation set
xgb_val_preds <- predict(xgb_model, newdata = xgb_val)

xgb_val_out <- matrix(xgb_val_preds, nrow = 8, ncol = length(xgb_val_preds) / 8) %>% 
               t() %>%
               data.frame() %>%
               mutate(max = max.col(., ties.method = "last"), label = val_labs + 1) 

# Confustion Matrix
xgb_val_conf <- table(true = val_labs + 1, pred = xgb_val_out$max)

cat("XGB Validation Classification Error Rate:", classification_error(xgb_val_conf), "\n")

```



```{r}

names(wish.train)
nrow(x_prep)
length(val_labs)
```





```{r}
data=select(wish.train, -c(sold_ct_3))
nrow(select(wish.train, -c(sold_ct_3)))
mat = model.matrix(select(wish.train, -c(sold_ct_3)))

```


```{r}


```



```{r}



```




```{r}



```


```{r}



```



```{r}



```



```{r}

```

